{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e1ae78",
   "metadata": {},
   "source": [
    "<h1>Módulo de Extracción de datos</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ead113",
   "metadata": {},
   "source": [
    "Este notebook tiene como finalidad realizar una recabación de twits relacionados al mundial 2022. Para realizar esta extracción de tweets utilizaremos las siguientes herramientas: Tweepy y Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6ec6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41fdb8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./twitter_keys.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "611adf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup access to API\n",
    "def connect_to_twitter_OAuth():\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)  # add proxy='172.30.1.251:6969' if needed\n",
    "    return api\n",
    "\n",
    "# Create API object\n",
    "api = connect_to_twitter_OAuth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0fa254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#contenido del tweet\n",
    "tweets = []\n",
    "#cantidad de likes del tweet\n",
    "likes = []\n",
    "#tiempo en el cual el tweet se creo\n",
    "time = []\n",
    "#ID del tweet\n",
    "tweet_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89af9c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add queries to search\n",
    "queries = ['Mundial','Qatar','#Qatar2022','Messi','Catar']\n",
    "rtfilter = \" -filter:retweets lang:es\"\n",
    "queries = [s + rtfilter for s in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3487ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in queries:\n",
    "    for tweet in tweepy.Cursor(api.search_tweets, \n",
    "                                     q = query).items(100):\n",
    "        text = tweet.text.replace('\\n',' ')\n",
    "        clean_tweet = re.sub(\"@[A-Za-z0-9]+\",\"\", text)\n",
    "        clean_tweet = re.sub(\"#[A-Za-z0-9]+\",\"\", clean_tweet)\n",
    "        clean_tweet = re.sub(r\"http\\S+\", \"\", clean_tweet)\n",
    "        tweets.append(clean_tweet.lower())\n",
    "        tweet_id.append(tweet.id_str)\n",
    "        likes.append(tweet.favorite_count)\n",
    "        time.append(tweet.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d458342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'tweet_id':tweet_id,'tweets':tweets,'likes':likes,'time':time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "160cc696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1594037474216509440</td>\n",
       "      <td>_18p   bueno, el físico no lo es todo. con cueva, yotún, carrillo, trauco fuimos…</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-19 18:38:44+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1594037472010313728</td>\n",
       "      <td>ya me veo que sale benzema, meten a tel y termina siendo la revelacion del mundial 😑</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-19 18:38:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1594037471544754181</td>\n",
       "      <td>lo de las críticas al mundial y el boom del periodismo. no vale de nada que se haga ahora. me parece populismo y de…</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-19 18:38:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1594037470416764928</td>\n",
       "      <td>jugar fútbol con los pibe'? claro que sí. pokémondisponible modo mundial.</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-19 18:38:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1594037468378300416</td>\n",
       "      <td>mundial de qatar: estalló la bronca entre fifa, boca y argentinos</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-19 18:38:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1594035276912857088</td>\n",
       "      <td>🇺🇾⚽ ¡uruguay ya llegó al mundial de catar 2022!   la celeste se prepara para su primer partido ante corea del sur 👇</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-19 18:30:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1594035276074156032</td>\n",
       "      <td>😏 ¿cómo están yendo estos primeros días en catar, ?  🛴⚡️ ¡¡estamos 𝘼 𝙏𝙊𝙊𝙊𝙊𝙋𝙀 para el mundial!!…</td>\n",
       "      <td>312</td>\n",
       "      <td>2022-11-19 18:30:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1594035276061368320</td>\n",
       "      <td>poco antes del inicio del mundial de fútbol de catar, activistas de derechos humanos, políticos y aficionados habla…</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-11-19 18:30:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1594035227055177728</td>\n",
       "      <td>el internacional brasileño sigue recordando sus últimos encuentros en liga en los que temió perderse el mundial por…</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-11-19 18:29:48+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1594035226455457799</td>\n",
       "      <td>2x1 catar</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-19 18:29:48+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id  \\\n",
       "0    1594037474216509440   \n",
       "1    1594037472010313728   \n",
       "2    1594037471544754181   \n",
       "3    1594037470416764928   \n",
       "4    1594037468378300416   \n",
       "..                   ...   \n",
       "495  1594035276912857088   \n",
       "496  1594035276074156032   \n",
       "497  1594035276061368320   \n",
       "498  1594035227055177728   \n",
       "499  1594035226455457799   \n",
       "\n",
       "                                                                                                                    tweets  \\\n",
       "0                                       _18p   bueno, el físico no lo es todo. con cueva, yotún, carrillo, trauco fuimos…    \n",
       "1                                     ya me veo que sale benzema, meten a tel y termina siendo la revelacion del mundial 😑   \n",
       "2    lo de las críticas al mundial y el boom del periodismo. no vale de nada que se haga ahora. me parece populismo y de…    \n",
       "3                                             jugar fútbol con los pibe'? claro que sí. pokémondisponible modo mundial.      \n",
       "4                                                      mundial de qatar: estalló la bronca entre fifa, boca y argentinos     \n",
       "..                                                                                                                     ...   \n",
       "495  🇺🇾⚽ ¡uruguay ya llegó al mundial de catar 2022!   la celeste se prepara para su primer partido ante corea del sur 👇     \n",
       "496                       😏 ¿cómo están yendo estos primeros días en catar, ?  🛴⚡️ ¡¡estamos 𝘼 𝙏𝙊𝙊𝙊𝙊𝙋𝙀 para el mundial!!…    \n",
       "497  poco antes del inicio del mundial de fútbol de catar, activistas de derechos humanos, políticos y aficionados habla…    \n",
       "498  el internacional brasileño sigue recordando sus últimos encuentros en liga en los que temió perderse el mundial por…    \n",
       "499                                                                                                              2x1 catar   \n",
       "\n",
       "     likes                      time  \n",
       "0        0 2022-11-19 18:38:44+00:00  \n",
       "1        0 2022-11-19 18:38:43+00:00  \n",
       "2        0 2022-11-19 18:38:43+00:00  \n",
       "3        0 2022-11-19 18:38:43+00:00  \n",
       "4        0 2022-11-19 18:38:42+00:00  \n",
       "..     ...                       ...  \n",
       "495      0 2022-11-19 18:30:00+00:00  \n",
       "496    312 2022-11-19 18:30:00+00:00  \n",
       "497      5 2022-11-19 18:30:00+00:00  \n",
       "498      1 2022-11-19 18:29:48+00:00  \n",
       "499      0 2022-11-19 18:29:48+00:00  \n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56586761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "df2.to_csv('D:\\\\FACU\\\\TF_CSVs\\\\'+'WorldCupTweets_' + str(now)[:16].replace(':','_') + '.csv',\n",
    "          index=True,\n",
    "          encoding='utf-8-sig',\n",
    "          sep=';'\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35d772",
   "metadata": {},
   "source": [
    "# Read all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da91fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import glob\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa56b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CSV files list from a folder\n",
    "path = 'D:\\FACU\\TF_CSVs'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "df_list = (pd.read_csv(file) for file in csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fef5b4d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 5, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Concatenate all DataFrames\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m big_df   \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Universidad\\Trabajo_Final\\venv\\env\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\Universidad\\Trabajo_Final\\venv\\env\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[0;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    1   3   4\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\Desktop\\Universidad\\Trabajo_Final\\venv\\env\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:422\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    420\u001b[0m     objs \u001b[38;5;241m=\u001b[39m [objs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys]\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 422\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [14], line 7\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m csv_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/*.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Read each CSV file into DataFrame\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# This creates a list of dataframes\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df_list \u001b[38;5;241m=\u001b[39m (\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m csv_files)\n",
      "File \u001b[1;32m~\\Desktop\\Universidad\\Trabajo_Final\\venv\\env\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\Universidad\\Trabajo_Final\\venv\\env\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\Universidad\\Trabajo_Final\\venv\\env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Universidad\\Trabajo_Final\\venv\\env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Universidad\\Trabajo_Final\\venv\\env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\Desktop\\Universidad\\Trabajo_Final\\venv\\env\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\Desktop\\Universidad\\Trabajo_Final\\venv\\env\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Desktop\\Universidad\\Trabajo_Final\\venv\\env\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Desktop\\Universidad\\Trabajo_Final\\venv\\env\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Desktop\\Universidad\\Trabajo_Final\\venv\\env\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 5, saw 3\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all DataFrames\n",
    "big_df   = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb21949",
   "metadata": {},
   "source": [
    "<h1> Dataset 1: Topic detection and sentiment analysis twitter dataset </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f667611",
   "metadata": {},
   "source": [
    "This is a Spanish dataset that is labeled for sentiment analysis and topic detection purposes. For more info: https://github.com/NatashaSvic/NLP_Spanish_Sentiment_Anaylsis_Text_Generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7eb87c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1591746",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('../0_datasets/TASS_data/general_corpus_2012/general-train-tagged-3l.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c47eb42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.DataFrame({'tweet_id':[],'tweetText':[],'polarity_value':[],'polarity_type':[],'topic':[]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "103fd6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "row=0\n",
    "for tweet in root:\n",
    "    tweet_id = 'ID:'+tweet.find('tweetid').text\n",
    "    #user = tweet.find('user').text\n",
    "    tweetText = tweet.find('content').text\n",
    "    lang = tweet.find('lang').text\n",
    "    polarity_value = tweet.find('sentiments').find('polarity').find('value').text\n",
    "    polarity_type = tweet.find('sentiments').find('polarity').find('type').text\n",
    "    topic = tweet.find('topics').find('topic').text\n",
    "    \n",
    "    if lang == 'es':\n",
    "        train_set.loc[row] = [tweet_id,tweetText,polarity_value,polarity_type,topic]\n",
    "        row+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6568dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>polarity_value</th>\n",
       "      <th>polarity_type</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID:142389495503925248</td>\n",
       "      <td>Salgo de #VeoTV , que día más largoooooo...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID:142389933619945473</td>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/nos. Besos y gracias</td>\n",
       "      <td>NEU</td>\n",
       "      <td>DISAGREEMENT</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID:142391947707940864</td>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID:142416095012339712</td>\n",
       "      <td>Off pensando en el regalito Sinde, la que se va de la SGAE cuando se van sus corruptos. Intento no sacar conclusiones (lo intento)</td>\n",
       "      <td>N</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>política</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID:142422495721562112</td>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ja te suena d algo!</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID:142424715175280640</td>\n",
       "      <td>RT @FabHddzC: Si amas a alguien, déjalo libre. Si grita ese hombre es mío era @paurubio...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>música</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID:142483342040907776</td>\n",
       "      <td>Toca @crackoviadeTV3 . Grabación dl especial Navideño...Mari crismas!</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>entretenimiento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ID:142493511634259968</td>\n",
       "      <td>Hoy asisitiré en Madrid a un seminario sobre la Estrategia Española de Seguridad organizado por FAES.</td>\n",
       "      <td>NONE</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>política</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ID:142494476051562496</td>\n",
       "      <td>Buen día todos! Lo primero mandar un abrazo grande a Miguel y a su familia @libertadmontes Hoy podría ser un día para la grandeza humana.</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ID:142496796416016384</td>\n",
       "      <td>Desde el escaño. Todo listo para empezar #endiascomohoy en el Congreso http://t.co/Mu2yIgCb</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>política</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id  \\\n",
       "0  ID:142389495503925248   \n",
       "1  ID:142389933619945473   \n",
       "2  ID:142391947707940864   \n",
       "3  ID:142416095012339712   \n",
       "4  ID:142422495721562112   \n",
       "5  ID:142424715175280640   \n",
       "6  ID:142483342040907776   \n",
       "7  ID:142493511634259968   \n",
       "8  ID:142494476051562496   \n",
       "9  ID:142496796416016384   \n",
       "\n",
       "                                                                                                                                   tweetText  \\\n",
       "0                                                                                                Salgo de #VeoTV , que día más largoooooo...   \n",
       "1                                                                          @PauladeLasHeras No te libraras de ayudar me/nos. Besos y gracias   \n",
       "2                                                                                                                  @marodriguezb Gracias MAR   \n",
       "3         Off pensando en el regalito Sinde, la que se va de la SGAE cuando se van sus corruptos. Intento no sacar conclusiones (lo intento)   \n",
       "4                                                                          Conozco a alguien q es adicto al drama! Ja ja ja te suena d algo!   \n",
       "5                                                 RT @FabHddzC: Si amas a alguien, déjalo libre. Si grita ese hombre es mío era @paurubio...   \n",
       "6                                                                      Toca @crackoviadeTV3 . Grabación dl especial Navideño...Mari crismas!   \n",
       "7                                      Hoy asisitiré en Madrid a un seminario sobre la Estrategia Española de Seguridad organizado por FAES.   \n",
       "8  Buen día todos! Lo primero mandar un abrazo grande a Miguel y a su familia @libertadmontes Hoy podría ser un día para la grandeza humana.   \n",
       "9                                                Desde el escaño. Todo listo para empezar #endiascomohoy en el Congreso http://t.co/Mu2yIgCb   \n",
       "\n",
       "  polarity_value polarity_type            topic  \n",
       "0           NONE     AGREEMENT            otros  \n",
       "1            NEU  DISAGREEMENT            otros  \n",
       "2              P     AGREEMENT            otros  \n",
       "3              N     AGREEMENT         política  \n",
       "4              P     AGREEMENT            otros  \n",
       "5           NONE     AGREEMENT           música  \n",
       "6              P     AGREEMENT  entretenimiento  \n",
       "7           NONE     AGREEMENT         política  \n",
       "8              P     AGREEMENT            otros  \n",
       "9              P     AGREEMENT         política  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66bea92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
