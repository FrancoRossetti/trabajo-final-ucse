{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e224db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Cacu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Cacu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "DEBUG:jupyter_black:config: {'line_length': 80, 'target_versions': {<TargetVersion.PY310: 10>}}\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            (function() {\n",
       "                jb_set_cell(\"# Create separate DataFrames for each class\\npositive_df = df[df[\\\"sentiment_encoded\\\"] == 2]\\nneutral_df = df[df[\\\"sentiment_encoded\\\"] == 1]\\nnegative_df = df[df[\\\"sentiment_encoded\\\"] == 0]\")\n",
       "            })();\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good!\n"
     ]
    }
   ],
   "source": [
    "import black\n",
    "import fasttext\n",
    "import jupyter_black\n",
    "import json\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Input, CuDNNLSTM, Dense\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import (\n",
    "    Bidirectional,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    GRU\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Setting options\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "#black for linting reasons\n",
    "jupyter_black.load(\n",
    "    lab=False,\n",
    "    line_length=80,\n",
    "    verbosity=\"DEBUG\",\n",
    "    target_version=black.TargetVersion.PY310,\n",
    ")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"All good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da65ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = r\"C:\\Users\\Cacu\\Desktop\\Universidad\\Trabajo_Final\\DataSets\\open-dataset-for-sentiment-analysis-master\\betsentiment-ES-tweets-sentiment-worldcup.csv\"\n",
    "\n",
    "\n",
    "# Function to extract sentiment scores from the json str\n",
    "def extract_sentiment_scores(json_str):\n",
    "    sentiment_data = json.loads(json_str)\n",
    "    return (\n",
    "        sentiment_data[\"Neutral\"],\n",
    "        sentiment_data[\"Negative\"],\n",
    "        sentiment_data[\"Positive\"],\n",
    "        sentiment_data[\"Mixed\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# Define data types\n",
    "dtype_dict = {\n",
    "    \"tweet_date_created\": str,\n",
    "    \"tweet_id\": int,\n",
    "    \"tweet_text\": str,\n",
    "    \"language\": str,\n",
    "    \"sentiment\": str,\n",
    "}\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path, encoding=\"utf-8\", dtype=dtype_dict)\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(csv_file_path, encoding=\"latin-1\", dtype=dtype_dict)\n",
    "\n",
    "sentiment_scores_list = df[\"sentiment_score\"].map(extract_sentiment_scores)\n",
    "\n",
    "sentiment_scores_df = pd.DataFrame(\n",
    "    sentiment_scores_list.tolist(),\n",
    "    columns=[\"Neutral\", \"Negative\", \"Positive\", \"Mixed\"],\n",
    ")\n",
    "\n",
    "df = pd.concat([df, sentiment_scores_df], axis=1)\n",
    "\n",
    "df.drop(\"sentiment_score\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d1e9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the stopwords from the .txt file and store them in a list\n",
    "custom_stopwords_file = (\n",
    "    r\"C:\\Users\\Cacu\\Desktop\\Universidad\\Trabajo_Final\\utilities\\stopwords.txt\"\n",
    ")\n",
    "with open(custom_stopwords_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    custom_stopwords_list = [line.strip() for line in file]\n",
    "\n",
    "# Create an empty set to hold the stopwords\n",
    "custom_stopwords_set = set()\n",
    "\n",
    "# Add the stopwords from the list to the set\n",
    "custom_stopwords_set.update(custom_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff324ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cacu\\AppData\\Local\\Temp\\ipykernel_12688\\3792906956.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"tweet_text\"] = df[\"tweet_text\"].str.replace(\"[^\\w\\s]\", \"\")\n"
     ]
    }
   ],
   "source": [
    "# Specify the Snowball stemmer for Spanish\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "# 01 - Convert NaN values to an empty string\n",
    "df[\"tweet_text\"] = df[\"tweet_text\"].fillna(\"\")\n",
    "\n",
    "# 02 - Lowercasing\n",
    "df[\"tweet_text\"] = df[\"tweet_text\"].str.lower()\n",
    "\n",
    "# 03 - Remove links (URLs)\n",
    "df[\"tweet_text\"] = df[\"tweet_text\"].apply(\n",
    "    lambda text: re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    ")\n",
    "\n",
    "# 04 - Removing punctuation\n",
    "df[\"tweet_text\"] = df[\"tweet_text\"].str.replace(\"[^\\w\\s]\", \"\")\n",
    "\n",
    "# 05 - Tokenization\n",
    "df[\"tokens\"] = df[\"tweet_text\"].apply(word_tokenize)\n",
    "\n",
    "# 06 - Removing stop words\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "df[\"filtered_tokens\"] = df[\"tokens\"].apply(\n",
    "    lambda tokens: [word for word in tokens if word not in stop_words]\n",
    ")\n",
    "\n",
    "# 07 - Stemming\n",
    "df[\"stemmed_tokens\"] = df[\"filtered_tokens\"].apply(\n",
    "    lambda tokens: [stemmer.stem(word) for word in tokens]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea60ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbddae94",
   "metadata": {},
   "source": [
    "## 1. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ac851",
   "metadata": {},
   "source": [
    "fastText es una biblioteca para el aprendizaje de incrustaciones de palabras y clasificación de texto creada por el laboratorio de Investigación de Inteligencia Artificial de Facebook (FAIR). El modelo es un algoritmo de aprendizaje no supervisado para obtener representaciones vectoriales de palabras. Facebook pone a disposición modelos preentrenados para 294 idiomas. fastText utiliza una red neuronal para la incrustación de palabras [Fuente: Wikipedia].\n",
    "\n",
    "Documentación sobre Gensim: models.fasttext\n",
    "\n",
    "FastText es una extensión de Word2Vec propuesta por Facebook en 2016. En lugar de alimentar palabras individuales en la Red Neuronal, FastText divide las palabras en varios n-gramas (subpalabras). Por ejemplo, los trigramas para la palabra \"manzana\" son \"man,\" \"anz,\" y \"nza\" (ignorando los límites de inicio y fin de las palabras). El vector de incrustación de la palabra \"manzana\" será la suma de todos estos n-gramas. Después de entrenar la Red Neuronal, tendremos incrustaciones de palabras para todos los n-gramas dados el conjunto de datos de entrenamiento. Las palabras raras ahora pueden representarse adecuadamente, ya que es muy probable que algunos de sus n-gramas también aparezcan en otras palabras. Te mostraré cómo usar FastText con Gensim en la siguiente sección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebed3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens_as_string\"] = df[\"tokens\"].apply(\" \".join)\n",
    "# Save the stemmed tokens to a text file (one sentence per line)\n",
    "with open(\"tokens.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(df[\"tokens_as_string\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef1a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fasttext model on stemmed tokens\n",
    "model = fasttext.train_unsupervised(\"tokens.txt\", model=\"skipgram\")\n",
    "\n",
    "# Create tweet-level embeddings using the trained model\n",
    "embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe396c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens in df[\"tokens_as_string\"]:\n",
    "    vector = model.get_sentence_vector(tokens)\n",
    "    embeddings.append(vector)\n",
    "\n",
    "# Convert the embeddings to a DataFrame\n",
    "embedding_df = pd.DataFrame(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54015993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings df with the original dataset\n",
    "df_worldcup_embeddings = pd.concat([df, embedding_df], axis=1)\n",
    "\n",
    "# embedding df to csv\n",
    "df_worldcup_embeddings.to_csv(\"2018_dataset_with_embeddings.csv\", index=False)\n",
    "\n",
    "model.save_model(\"model_worldcup_embedding.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72726ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_worldcup_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows_to_load = 1000000  # Adjust this to the desired subset size\n",
    "\n",
    "# start from here - csv already generated\n",
    "df = pd.read_csv(\"2018_dataset_with_embeddings.csv\", nrows=nrows_to_load)\n",
    "# shuffle the DataFrame rows\n",
    "# df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9306a6",
   "metadata": {},
   "source": [
    "## Preparing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23ba7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d02fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\"POSITIVE\": 2, \"NEUTRAL\": 1, \"NEGATIVE\": 0}\n",
    "df[\"sentiment_encoded\"] = df[\"sentiment\"].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c41cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate DataFrames for each class\n",
    "positive_df = df[df[\"sentiment_encoded\"] == 2]\n",
    "neutral_df = df[df[\"sentiment_encoded\"] == 1]\n",
    "negative_df = df[df[\"sentiment_encoded\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "992e8421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_date_created       0\n",
      "tweet_id                 0\n",
      "tweet_text               0\n",
      "language                 0\n",
      "sentiment                0\n",
      "Neutral                  0\n",
      "Negative                 0\n",
      "Positive                 0\n",
      "Mixed                    0\n",
      "tokens                   0\n",
      "filtered_tokens          0\n",
      "stemmed_tokens           0\n",
      "sentiment_encoded     7606\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(subset=[\"tweet_text\"], keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5db2f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 25k rows from each class\n",
    "num_samples = 25000\n",
    "positive_sample = positive_df.sample(n=num_samples, random_state=42)\n",
    "neutral_sample = neutral_df.sample(n=num_samples, random_state=42)\n",
    "negative_sample = negative_df.sample(n=num_samples, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d848b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the samples to create a balanced subset\n",
    "subset_df = pd.concat([positive_sample, neutral_sample, negative_sample])\n",
    "\n",
    "# Shuffle the subset to randomize the order\n",
    "subset_df = subset_df.sample(frac=1, random_state=42)\n",
    "\n",
    "df = subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be927143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract X (features) and y (target)\n",
    "X = df.iloc[:, 13:-1].to_numpy()\n",
    "y = df[\"sentiment_encoded\"].to_numpy()\n",
    "\n",
    "# Make sure y is in the correct shape\n",
    "y = np.reshape(y, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64bc4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbafde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1e14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# Fit the tokenizer on your tokenized text data\n",
    "tokenizer.fit_on_texts(df[\"tokens\"])\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for the zero padding\n",
    "\n",
    "# Print the vocabulary size\n",
    "print(\"Vocabulary size (input_dim):\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM\n",
    "\n",
    "# Define the model\n",
    "model2 = Sequential()\n",
    "model2.add(\n",
    "    Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=100,\n",
    "        input_length=100,\n",
    "    )\n",
    ")\n",
    "model2.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model2.add(Bidirectional(LSTM(100)))\n",
    "model2.add(Dense(128,activation='tanh'))\n",
    "model2.add(\n",
    "    Dropout(0.2)\n",
    ")\n",
    "model2.add(Dense(32,activation='relu'))\n",
    "model2.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca6e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model2.compile(\n",
    "    loss=SparseCategoricalCrossentropy(),\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model2.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    validation_split=0.2,\n",
    "#    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model2.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss}, Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tirar los word embedding post traerse el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900205cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'cv'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35310b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'cv'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = model.predict(X_test)\n",
    "y_pred = []\n",
    "for i in range(prob.shape[0]):\n",
    "  if(prob[i][0]>=0.5):\n",
    "    y_pred.append(1)\n",
    "  else:\n",
    "    y_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ff875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other word embedding - since fasttext unsupervised model not working as expected\n",
    "import fasttext.util\n",
    "\n",
    "# fasttext.util.download_model('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ea6098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText word embeddings model\n",
    "model = fasttext.load_model(\"cc.es.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bf3bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 150000\n",
    "random_subset = df.sample(\n",
    "    n=subset_size, random_state=42\n",
    ")  # Adjust the random_state as needed\n",
    "\n",
    "df = random_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce16153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e3c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty NumPy array\n",
    "empty_array = np.empty((len(df),), dtype=object)\n",
    "\n",
    "# Assign the empty array to a new column in your DataFrame\n",
    "df[\"vector\"] = empty_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.index:\n",
    "    # Calculate the embedding for each token in the tweet\n",
    "    tweet_vector = np.mean(\n",
    "        [model.get_word_vector(token) for token in df[\"tokens\"][row]], axis=0\n",
    "    )\n",
    "\n",
    "    # Store the tweet embeddings in the dataframe\n",
    "    df[\"vector\"][row] = tweet_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06250b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_word_vector(\"Messi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5168546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\"POSITIVE\": 2, \"NEUTRAL\": 1, \"NEGATIVE\": 0}\n",
    "df[\"sentiment_encoded\"] = df[\"sentiment\"].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d04b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: separate DataFrames for each class\n",
    "positive_df = df[df['sentiment_encoded'] == 2]\n",
    "neutral_df = df[df['sentiment_encoded'] == 1]\n",
    "negative_df = df[df['sentiment_encoded'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d247d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 25k rows from each class\n",
    "num_samples = 25000\n",
    "positive_sample = positive_df.sample(n=num_samples, random_state=42)\n",
    "neutral_sample = neutral_df.sample(n=num_samples, random_state=42)\n",
    "negative_sample = negative_df.sample(n=num_samples, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(subset=[\"tweet_text\"], keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b83b7650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging, os\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "    df[\"tokens\"], min_count=10, vector_size=100, workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6077b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"MODEL_WORD2VEC.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b5322c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000\n"
     ]
    }
   ],
   "source": [
    "print(model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50f44ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary based on the vectors\n",
    "vocab = {word: index for index, word in enumerate(model.wv.index_to_key)}\n",
    "\n",
    "# Define the index for the 'unknown' token\n",
    "unknown_token_index = len(vocab)  # Assuming it's the next index\n",
    "\n",
    "\n",
    "# Function to map words to indices, with handling for out-of-vocabulary words\n",
    "def words_to_indices(vec):\n",
    "    return [vocab.get(word, unknown_token_index) for word in vec]\n",
    "\n",
    "\n",
    "# Apply the mapping function to your 'tokens' column\n",
    "df[\"sequence\"] = df[\"tokens\"].apply(words_to_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c38ccfff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_date_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>language</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Mixed</th>\n",
       "      <th>tokens</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107374</th>\n",
       "      <td>2018-07-02T12:40:07.432000</td>\n",
       "      <td>-740831232</td>\n",
       "      <td>buenos días a todos \\noye miseleccionmx acá creímos y sí pudimos también creemos que van a ganar ahora les toca a ustedes creer y vencer \\njuegaméxico</td>\n",
       "      <td>es</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>0.640794</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.321520</td>\n",
       "      <td>0.024786</td>\n",
       "      <td>[buenos, días, a, todos, oye, miseleccionmx, acá, creímos, y, sí, pudimos, también, creemos, que, van, a, ganar, ahora, les, toca, a, ustedes, creer, y, vencer, juegaméxico]</td>\n",
       "      <td>[buenos, días, oye, miseleccionmx, acá, creímos, pudimos, creemos, van, ganar, ahora, toca, ustedes, creer, vencer, juegaméxico]</td>\n",
       "      <td>[buen, dias, oye, miseleccionmx, aca, creim, pud, cre, van, gan, ahor, toc, usted, cre, venc, juegamex]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[244, 181, 2, 41, 1232, 8, 445, 9315, 4, 198, 4451, 152, 1395, 1, 110, 2, 86, 78, 74, 546, 2, 84, 556, 4, 2176, 1084]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826058</th>\n",
       "      <td>2018-05-08T20:01:17</td>\n",
       "      <td>492277760</td>\n",
       "      <td>record_mexico miseleccionmx tiene toda la razon el piojo miguelherreradt  es un momento vital para ver el nivel de los mejores en el futbol mexicano a 38 días del mundial</td>\n",
       "      <td>es</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.466427</td>\n",
       "      <td>0.020149</td>\n",
       "      <td>0.495606</td>\n",
       "      <td>0.017818</td>\n",
       "      <td>[record_mexico, miseleccionmx, tiene, toda, la, razon, el, piojo, miguelherreradt, es, un, momento, vital, para, ver, el, nivel, de, los, mejores, en, el, futbol, mexicano, a, 38, días, del, mundial]</td>\n",
       "      <td>[record_mexico, miseleccionmx, toda, razon, piojo, miguelherreradt, momento, vital, ver, nivel, mejores, futbol, mexicano, 38, días, mundial]</td>\n",
       "      <td>[record_mex, miseleccionmx, tod, razon, pioj, miguelherreradt, moment, vital, ver, nivel, mejor, futbol, mexican, 38, dias, mundial]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[816, 8, 62, 117, 3, 3213, 5, 3475, 3286, 9, 15, 186, 9315, 16, 60, 5, 284, 0, 10, 192, 7, 5, 230, 325, 2, 5205, 181, 20, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98555</th>\n",
       "      <td>2018-06-12T03:18:53</td>\n",
       "      <td>823517185</td>\n",
       "      <td>miseleccionmx marcofabian_10 que bien ya las escortos saben a donde ir gran detalle</td>\n",
       "      <td>es</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.114220</td>\n",
       "      <td>0.021354</td>\n",
       "      <td>0.810466</td>\n",
       "      <td>0.053959</td>\n",
       "      <td>[miseleccionmx, marcofabian_10, que, bien, ya, las, escortos, saben, a, donde, ir, gran, detalle]</td>\n",
       "      <td>[miseleccionmx, marcofabian_10, bien, escortos, saben, ir, gran, detalle]</td>\n",
       "      <td>[miseleccionmx, marcofabian_10, bien, escort, sab, ir, gran, detall]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[8, 477, 1, 52, 27, 33, 9315, 391, 2, 201, 177, 91, 2410]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240234</th>\n",
       "      <td>2018-06-26T16:03:45.065000</td>\n",
       "      <td>1293389825</td>\n",
       "      <td>argentina eso espero los amigos de messi no vayan a llorar si quedan afuera póngan lo que no tienen</td>\n",
       "      <td>es</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.423679</td>\n",
       "      <td>0.431658</td>\n",
       "      <td>0.061877</td>\n",
       "      <td>0.082786</td>\n",
       "      <td>[argentina, eso, espero, los, amigos, de, messi, no, vayan, a, llorar, si, quedan, afuera, póngan, lo, que, no, tienen]</td>\n",
       "      <td>[argentina, espero, amigos, messi, vayan, llorar, si, quedan, afuera, póngan]</td>\n",
       "      <td>[argentin, esper, amig, messi, vay, llor, si, qued, afuer, pong]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[17, 66, 204, 10, 450, 0, 100, 6, 547, 2, 626, 21, 1028, 677, 9315, 14, 1, 6, 124]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104961</th>\n",
       "      <td>2018-07-02T03:43:36.696000</td>\n",
       "      <td>-694767616</td>\n",
       "      <td>que miseleccionmx no me parta el corazón mañana  que ando malita después de hoy</td>\n",
       "      <td>es</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.279599</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.074039</td>\n",
       "      <td>0.086838</td>\n",
       "      <td>[que, miseleccionmx, no, me, parta, el, corazón, mañana, que, ando, malita, después, de, hoy]</td>\n",
       "      <td>[miseleccionmx, parta, corazón, mañana, ando, malita, después, hoy]</td>\n",
       "      <td>[miseleccionmx, part, corazon, mañan, ando, malit, despues, hoy]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 8, 6, 22, 9315, 5, 213, 81, 1, 2805, 9315, 200, 0, 35]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381994</th>\n",
       "      <td>2018-06-22T19:13:32</td>\n",
       "      <td>123179014</td>\n",
       "      <td>rusia2018 la derrota argentina reflejada en las portadas de los diarios de todo el mundo</td>\n",
       "      <td>es</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>0.943339</td>\n",
       "      <td>0.009795</td>\n",
       "      <td>0.042435</td>\n",
       "      <td>0.004431</td>\n",
       "      <td>[rusia2018, la, derrota, argentina, reflejada, en, las, portadas, de, los, diarios, de, todo, el, mundo]</td>\n",
       "      <td>[rusia2018, derrota, argentina, reflejada, portadas, diarios, mundo]</td>\n",
       "      <td>[rusia2018, derrot, argentin, reflej, port, diari, mund]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[31, 3, 959, 17, 9315, 7, 33, 9315, 0, 10, 7869, 0, 29, 5, 92]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13063</th>\n",
       "      <td>2018-05-28T22:46:29</td>\n",
       "      <td>-1118453760</td>\n",
       "      <td>miseleccionmx cocacolamx con sarahpernight porque es la quien me cae bien jaja diacocacola miseleccionmx cocacolamx \\nfan id 42745</td>\n",
       "      <td>es</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.341573</td>\n",
       "      <td>0.083629</td>\n",
       "      <td>0.373011</td>\n",
       "      <td>0.201786</td>\n",
       "      <td>[miseleccionmx, cocacolamx, con, sarahpernight, porque, es, la, quien, me, cae, bien, jaja, diacocacola, miseleccionmx, cocacolamx, fan, id, 42745]</td>\n",
       "      <td>[miseleccionmx, cocacolamx, sarahpernight, cae, bien, jaja, diacocacola, miseleccionmx, cocacolamx, fan, id, 42745]</td>\n",
       "      <td>[miseleccionmx, cocacolamx, sarahpernight, cae, bien, jaj, diacocacol, miseleccionmx, cocacolamx, fan, id, 42745]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[8, 1086, 13, 9315, 70, 9, 3, 208, 22, 1337, 52, 560, 9315, 8, 1086, 492, 648, 9315]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485463</th>\n",
       "      <td>2018-06-17T17:03:53</td>\n",
       "      <td>-1194037247</td>\n",
       "      <td>ey no manches miseleccionmx no tengo memes para la victoria</td>\n",
       "      <td>es</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.304465</td>\n",
       "      <td>0.594887</td>\n",
       "      <td>0.039803</td>\n",
       "      <td>0.060846</td>\n",
       "      <td>[ey, no, manches, miseleccionmx, no, tengo, memes, para, la, victoria]</td>\n",
       "      <td>[ey, manches, miseleccionmx, memes, victoria]</td>\n",
       "      <td>[ey, manch, miseleccionmx, mem, victori]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[9315, 6, 4653, 8, 6, 202, 2278, 16, 3, 318]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494740</th>\n",
       "      <td>2018-06-24T21:33:54</td>\n",
       "      <td>-971579387</td>\n",
       "      <td>cuadrado falcao fundajcuadrado fifaworldcup_es fifaworldcup fcfseleccioncol cuadrado falcao y toda mi selección tricolor son los mejores  gracias por alegran ntros  dios los bendiga siempre y ganen todos los partidos</td>\n",
       "      <td>es</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.223495</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.769663</td>\n",
       "      <td>0.006281</td>\n",
       "      <td>[cuadrado, falcao, fundajcuadrado, fifaworldcup_es, fifaworldcup, fcfseleccioncol, cuadrado, falcao, y, toda, mi, selección, tricolor, son, los, mejores, gracias, por, alegran, ntros, dios, los, bendiga, siempre, y, ganen, todos, los, partidos]</td>\n",
       "      <td>[cuadrado, falcao, fundajcuadrado, fifaworldcup_es, fifaworldcup, fcfseleccioncol, cuadrado, falcao, toda, selección, tricolor, mejores, gracias, alegran, ntros, dios, bendiga, siempre, ganen, partidos]</td>\n",
       "      <td>[cuadr, falca, fundajcuadr, fifaworldcup_, fifaworldcup, fcfseleccioncol, cuadr, falca, tod, seleccion, tricolor, mejor, graci, alegr, ntros, dios, bendig, siempr, gan, part]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[278, 139, 2424, 89, 103, 18, 278, 139, 4, 117, 32, 36, 773, 54, 10, 192, 47, 12, 4050, 9315, 126, 10, 515, 68, 4, 532, 41, 10, 159]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557921</th>\n",
       "      <td>2018-06-20T14:52:17</td>\n",
       "      <td>-1726488575</td>\n",
       "      <td>somos la selección más fuerte y sin lugar a dudas la favorita españa pasará por encima de irán y lo disfrutaré en vodafonetv_es vamosespaña todoestáporver\\nhoy \\n       \\n         i sí o sí</td>\n",
       "      <td>es</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.356602</td>\n",
       "      <td>0.016170</td>\n",
       "      <td>0.599672</td>\n",
       "      <td>0.027557</td>\n",
       "      <td>[somos, la, selección, más, fuerte, y, sin, lugar, a, dudas, la, favorita, españa, pasará, por, encima, de, irán, y, lo, disfrutaré, en, vodafonetv_es, vamosespaña, todoestáporver, hoy, i, sí, o, sí]</td>\n",
       "      <td>[selección, fuerte, lugar, dudas, favorita, españa, pasará, encima, irán, disfrutaré, vodafonetv_es, vamosespaña, todoestáporver, hoy, i]</td>\n",
       "      <td>[seleccion, fuert, lug, dud, favorit, españ, pas, encim, iran, disfrut, vodafonetv_, vamosespañ, todoestaporv, hoy, i]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[123, 3, 36, 28, 324, 4, 73, 219, 2, 417, 3, 539, 134, 576, 12, 315, 0, 400, 4, 14, 678, 7, 706, 150, 707, 35, 1581, 198, 56, 198]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_date_created    tweet_id  \\\n",
       "107374  2018-07-02T12:40:07.432000  -740831232   \n",
       "826058         2018-05-08T20:01:17   492277760   \n",
       "98555          2018-06-12T03:18:53   823517185   \n",
       "240234  2018-06-26T16:03:45.065000  1293389825   \n",
       "104961  2018-07-02T03:43:36.696000  -694767616   \n",
       "...                            ...         ...   \n",
       "381994         2018-06-22T19:13:32   123179014   \n",
       "13063          2018-05-28T22:46:29 -1118453760   \n",
       "485463         2018-06-17T17:03:53 -1194037247   \n",
       "494740         2018-06-24T21:33:54  -971579387   \n",
       "557921         2018-06-20T14:52:17 -1726488575   \n",
       "\n",
       "                                                                                                                                                                                                                       tweet_text  \\\n",
       "107374                                                                     buenos días a todos \\noye miseleccionmx acá creímos y sí pudimos también creemos que van a ganar ahora les toca a ustedes creer y vencer \\njuegaméxico   \n",
       "826058                                                 record_mexico miseleccionmx tiene toda la razon el piojo miguelherreradt  es un momento vital para ver el nivel de los mejores en el futbol mexicano a 38 días del mundial   \n",
       "98555                                                                                                                                         miseleccionmx marcofabian_10 que bien ya las escortos saben a donde ir gran detalle   \n",
       "240234                                                                                                                        argentina eso espero los amigos de messi no vayan a llorar si quedan afuera póngan lo que no tienen   \n",
       "104961                                                                                                                                            que miseleccionmx no me parta el corazón mañana  que ando malita después de hoy   \n",
       "...                                                                                                                                                                                                                           ...   \n",
       "381994                                                                                                                                rusia2018 la derrota argentina reflejada en las portadas de los diarios de todo el mundo      \n",
       "13063                                                                                          miseleccionmx cocacolamx con sarahpernight porque es la quien me cae bien jaja diacocacola miseleccionmx cocacolamx \\nfan id 42745   \n",
       "485463                                                                                                                                                                ey no manches miseleccionmx no tengo memes para la victoria   \n",
       "494740  cuadrado falcao fundajcuadrado fifaworldcup_es fifaworldcup fcfseleccioncol cuadrado falcao y toda mi selección tricolor son los mejores  gracias por alegran ntros  dios los bendiga siempre y ganen todos los partidos    \n",
       "557921                              somos la selección más fuerte y sin lugar a dudas la favorita españa pasará por encima de irán y lo disfrutaré en vodafonetv_es vamosespaña todoestáporver\\nhoy \\n       \\n         i sí o sí   \n",
       "\n",
       "       language sentiment   Neutral  Negative  Positive     Mixed  \\\n",
       "107374       es   NEUTRAL  0.640794  0.012900  0.321520  0.024786   \n",
       "826058       es  POSITIVE  0.466427  0.020149  0.495606  0.017818   \n",
       "98555        es  POSITIVE  0.114220  0.021354  0.810466  0.053959   \n",
       "240234       es  NEGATIVE  0.423679  0.431658  0.061877  0.082786   \n",
       "104961       es  NEGATIVE  0.279599  0.559524  0.074039  0.086838   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "381994       es   NEUTRAL  0.943339  0.009795  0.042435  0.004431   \n",
       "13063        es  POSITIVE  0.341573  0.083629  0.373011  0.201786   \n",
       "485463       es  NEGATIVE  0.304465  0.594887  0.039803  0.060846   \n",
       "494740       es  POSITIVE  0.223495  0.000561  0.769663  0.006281   \n",
       "557921       es  POSITIVE  0.356602  0.016170  0.599672  0.027557   \n",
       "\n",
       "                                                                                                                                                                                                                                                      tokens  \\\n",
       "107374                                                                         [buenos, días, a, todos, oye, miseleccionmx, acá, creímos, y, sí, pudimos, también, creemos, que, van, a, ganar, ahora, les, toca, a, ustedes, creer, y, vencer, juegaméxico]   \n",
       "826058                                               [record_mexico, miseleccionmx, tiene, toda, la, razon, el, piojo, miguelherreradt, es, un, momento, vital, para, ver, el, nivel, de, los, mejores, en, el, futbol, mexicano, a, 38, días, del, mundial]   \n",
       "98555                                                                                                                                                      [miseleccionmx, marcofabian_10, que, bien, ya, las, escortos, saben, a, donde, ir, gran, detalle]   \n",
       "240234                                                                                                                               [argentina, eso, espero, los, amigos, de, messi, no, vayan, a, llorar, si, quedan, afuera, póngan, lo, que, no, tienen]   \n",
       "104961                                                                                                                                                         [que, miseleccionmx, no, me, parta, el, corazón, mañana, que, ando, malita, después, de, hoy]   \n",
       "...                                                                                                                                                                                                                                                      ...   \n",
       "381994                                                                                                                                              [rusia2018, la, derrota, argentina, reflejada, en, las, portadas, de, los, diarios, de, todo, el, mundo]   \n",
       "13063                                                                                                    [miseleccionmx, cocacolamx, con, sarahpernight, porque, es, la, quien, me, cae, bien, jaja, diacocacola, miseleccionmx, cocacolamx, fan, id, 42745]   \n",
       "485463                                                                                                                                                                                [ey, no, manches, miseleccionmx, no, tengo, memes, para, la, victoria]   \n",
       "494740  [cuadrado, falcao, fundajcuadrado, fifaworldcup_es, fifaworldcup, fcfseleccioncol, cuadrado, falcao, y, toda, mi, selección, tricolor, son, los, mejores, gracias, por, alegran, ntros, dios, los, bendiga, siempre, y, ganen, todos, los, partidos]   \n",
       "557921                                               [somos, la, selección, más, fuerte, y, sin, lugar, a, dudas, la, favorita, españa, pasará, por, encima, de, irán, y, lo, disfrutaré, en, vodafonetv_es, vamosespaña, todoestáporver, hoy, i, sí, o, sí]   \n",
       "\n",
       "                                                                                                                                                                                                   filtered_tokens  \\\n",
       "107374                                                                            [buenos, días, oye, miseleccionmx, acá, creímos, pudimos, creemos, van, ganar, ahora, toca, ustedes, creer, vencer, juegaméxico]   \n",
       "826058                                                               [record_mexico, miseleccionmx, toda, razon, piojo, miguelherreradt, momento, vital, ver, nivel, mejores, futbol, mexicano, 38, días, mundial]   \n",
       "98555                                                                                                                                    [miseleccionmx, marcofabian_10, bien, escortos, saben, ir, gran, detalle]   \n",
       "240234                                                                                                                               [argentina, espero, amigos, messi, vayan, llorar, si, quedan, afuera, póngan]   \n",
       "104961                                                                                                                                         [miseleccionmx, parta, corazón, mañana, ando, malita, después, hoy]   \n",
       "...                                                                                                                                                                                                            ...   \n",
       "381994                                                                                                                                        [rusia2018, derrota, argentina, reflejada, portadas, diarios, mundo]   \n",
       "13063                                                                                          [miseleccionmx, cocacolamx, sarahpernight, cae, bien, jaja, diacocacola, miseleccionmx, cocacolamx, fan, id, 42745]   \n",
       "485463                                                                                                                                                               [ey, manches, miseleccionmx, memes, victoria]   \n",
       "494740  [cuadrado, falcao, fundajcuadrado, fifaworldcup_es, fifaworldcup, fcfseleccioncol, cuadrado, falcao, toda, selección, tricolor, mejores, gracias, alegran, ntros, dios, bendiga, siempre, ganen, partidos]   \n",
       "557921                                                                   [selección, fuerte, lugar, dudas, favorita, españa, pasará, encima, irán, disfrutaré, vodafonetv_es, vamosespaña, todoestáporver, hoy, i]   \n",
       "\n",
       "                                                                                                                                                                        stemmed_tokens  \\\n",
       "107374                                                                         [buen, dias, oye, miseleccionmx, aca, creim, pud, cre, van, gan, ahor, toc, usted, cre, venc, juegamex]   \n",
       "826058                                            [record_mex, miseleccionmx, tod, razon, pioj, miguelherreradt, moment, vital, ver, nivel, mejor, futbol, mexican, 38, dias, mundial]   \n",
       "98555                                                                                                             [miseleccionmx, marcofabian_10, bien, escort, sab, ir, gran, detall]   \n",
       "240234                                                                                                                [argentin, esper, amig, messi, vay, llor, si, qued, afuer, pong]   \n",
       "104961                                                                                                                [miseleccionmx, part, corazon, mañan, ando, malit, despues, hoy]   \n",
       "...                                                                                                                                                                                ...   \n",
       "381994                                                                                                                        [rusia2018, derrot, argentin, reflej, port, diari, mund]   \n",
       "13063                                                                [miseleccionmx, cocacolamx, sarahpernight, cae, bien, jaj, diacocacol, miseleccionmx, cocacolamx, fan, id, 42745]   \n",
       "485463                                                                                                                                        [ey, manch, miseleccionmx, mem, victori]   \n",
       "494740  [cuadr, falca, fundajcuadr, fifaworldcup_, fifaworldcup, fcfseleccioncol, cuadr, falca, tod, seleccion, tricolor, mejor, graci, alegr, ntros, dios, bendig, siempr, gan, part]   \n",
       "557921                                                          [seleccion, fuert, lug, dud, favorit, españ, pas, encim, iran, disfrut, vodafonetv_, vamosespañ, todoestaporv, hoy, i]   \n",
       "\n",
       "        sentiment_encoded  \\\n",
       "107374                1.0   \n",
       "826058                2.0   \n",
       "98555                 2.0   \n",
       "240234                0.0   \n",
       "104961                0.0   \n",
       "...                   ...   \n",
       "381994                1.0   \n",
       "13063                 2.0   \n",
       "485463                0.0   \n",
       "494740                2.0   \n",
       "557921                2.0   \n",
       "\n",
       "                                                                                                                                    sequence  \n",
       "107374                 [244, 181, 2, 41, 1232, 8, 445, 9315, 4, 198, 4451, 152, 1395, 1, 110, 2, 86, 78, 74, 546, 2, 84, 556, 4, 2176, 1084]  \n",
       "826058         [816, 8, 62, 117, 3, 3213, 5, 3475, 3286, 9, 15, 186, 9315, 16, 60, 5, 284, 0, 10, 192, 7, 5, 230, 325, 2, 5205, 181, 20, 30]  \n",
       "98555                                                                              [8, 477, 1, 52, 27, 33, 9315, 391, 2, 201, 177, 91, 2410]  \n",
       "240234                                                    [17, 66, 204, 10, 450, 0, 100, 6, 547, 2, 626, 21, 1028, 677, 9315, 14, 1, 6, 124]  \n",
       "104961                                                                            [1, 8, 6, 22, 9315, 5, 213, 81, 1, 2805, 9315, 200, 0, 35]  \n",
       "...                                                                                                                                      ...  \n",
       "381994                                                                        [31, 3, 959, 17, 9315, 7, 33, 9315, 0, 10, 7869, 0, 29, 5, 92]  \n",
       "13063                                                   [8, 1086, 13, 9315, 70, 9, 3, 208, 22, 1337, 52, 560, 9315, 8, 1086, 492, 648, 9315]  \n",
       "485463                                                                                          [9315, 6, 4653, 8, 6, 202, 2278, 16, 3, 318]  \n",
       "494740  [278, 139, 2424, 89, 103, 18, 278, 139, 4, 117, 32, 36, 773, 54, 10, 192, 47, 12, 4050, 9315, 126, 10, 515, 68, 4, 532, 41, 10, 159]  \n",
       "557921    [123, 3, 36, 28, 324, 4, 73, 219, 2, 417, 3, 539, 134, 576, 12, 315, 0, 400, 4, 14, 678, 7, 706, 150, 707, 35, 1581, 198, 56, 198]  \n",
       "\n",
       "[75000 rows x 14 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad6e428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract X (features) and y (target)\n",
    "X = df[\"sequence\"]\n",
    "y = df[\"sentiment_encoded\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f1660dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 2., ..., 0., 2., 2.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad8ab27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y in the correct shape\n",
    "y = np.reshape(y, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce5599bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "for tokens in df[\"tokens\"]:\n",
    "    vocabulary.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff097063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 143778\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 143778\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7336215",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 40\n",
    "X = pad_sequences(X, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c302c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e427cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM\n",
    "\n",
    "# Define the model\n",
    "model2 = Sequential()\n",
    "model2.add(\n",
    "    Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=100,\n",
    "        input_length=40,\n",
    "    )\n",
    ")\n",
    "model2.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model2.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model2.add(Bidirectional(LSTM(100)))\n",
    "model2.add(Dense(128, activation=\"tanh\"))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(32, activation=\"relu\"))\n",
    "model2.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d84c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "188/188 [==============================] - 86s 425ms/step - loss: 0.6926 - accuracy: 0.6882 - val_loss: 0.5439 - val_accuracy: 0.7744\n",
      "Epoch 2/30\n",
      "188/188 [==============================] - 78s 414ms/step - loss: 0.4502 - accuracy: 0.8240 - val_loss: 0.5310 - val_accuracy: 0.7899\n",
      "Epoch 3/30\n",
      "188/188 [==============================] - 78s 414ms/step - loss: 0.3818 - accuracy: 0.8533 - val_loss: 0.5711 - val_accuracy: 0.7825\n",
      "Epoch 4/30\n",
      "188/188 [==============================] - 78s 416ms/step - loss: 0.3328 - accuracy: 0.8738 - val_loss: 0.6061 - val_accuracy: 0.7765\n",
      "Epoch 5/30\n",
      "188/188 [==============================] - 78s 418ms/step - loss: 0.2867 - accuracy: 0.8939 - val_loss: 0.6291 - val_accuracy: 0.7698\n",
      "Epoch 6/30\n",
      "188/188 [==============================] - 78s 416ms/step - loss: 0.2408 - accuracy: 0.9137 - val_loss: 0.7040 - val_accuracy: 0.7679\n",
      "Epoch 7/30\n",
      "188/188 [==============================] - 78s 415ms/step - loss: 0.1999 - accuracy: 0.9292 - val_loss: 0.7810 - val_accuracy: 0.7616\n",
      "Epoch 8/30\n",
      "188/188 [==============================] - 78s 413ms/step - loss: 0.1686 - accuracy: 0.9399 - val_loss: 0.9332 - val_accuracy: 0.7561\n",
      "Epoch 9/30\n",
      "188/188 [==============================] - 78s 416ms/step - loss: 0.1426 - accuracy: 0.9491 - val_loss: 0.9530 - val_accuracy: 0.7556\n",
      "Epoch 10/30\n",
      "188/188 [==============================] - 78s 416ms/step - loss: 0.1234 - accuracy: 0.9574 - val_loss: 1.0471 - val_accuracy: 0.7475\n",
      "Epoch 11/30\n",
      "188/188 [==============================] - 78s 414ms/step - loss: 0.1106 - accuracy: 0.9612 - val_loss: 1.1164 - val_accuracy: 0.7506\n",
      "Epoch 12/30\n",
      "188/188 [==============================] - 75s 399ms/step - loss: 0.0929 - accuracy: 0.9675 - val_loss: 1.2656 - val_accuracy: 0.7496\n",
      "Epoch 13/30\n",
      "188/188 [==============================] - 73s 388ms/step - loss: 0.0813 - accuracy: 0.9715 - val_loss: 1.2465 - val_accuracy: 0.7418\n",
      "Epoch 14/30\n",
      "188/188 [==============================] - 73s 387ms/step - loss: 0.0689 - accuracy: 0.9750 - val_loss: 1.4866 - val_accuracy: 0.7473\n",
      "Epoch 15/30\n",
      "188/188 [==============================] - 73s 387ms/step - loss: 0.0600 - accuracy: 0.9779 - val_loss: 1.5473 - val_accuracy: 0.7437\n",
      "Epoch 16/30\n",
      "188/188 [==============================] - 73s 386ms/step - loss: 0.0572 - accuracy: 0.9785 - val_loss: 1.4832 - val_accuracy: 0.7437\n",
      "Epoch 17/30\n",
      "188/188 [==============================] - 73s 386ms/step - loss: 0.0481 - accuracy: 0.9825 - val_loss: 1.5782 - val_accuracy: 0.7457\n",
      "Epoch 18/30\n",
      "188/188 [==============================] - 73s 386ms/step - loss: 0.0475 - accuracy: 0.9824 - val_loss: 1.5133 - val_accuracy: 0.7492\n",
      "Epoch 19/30\n",
      "188/188 [==============================] - 73s 386ms/step - loss: 0.0387 - accuracy: 0.9856 - val_loss: 1.6718 - val_accuracy: 0.7452\n",
      "Epoch 20/30\n",
      "188/188 [==============================] - 72s 386ms/step - loss: 0.0396 - accuracy: 0.9858 - val_loss: 1.5455 - val_accuracy: 0.7477\n",
      "Epoch 21/30\n",
      "188/188 [==============================] - 72s 385ms/step - loss: 0.0350 - accuracy: 0.9872 - val_loss: 1.7115 - val_accuracy: 0.7408\n",
      "Epoch 22/30\n",
      "188/188 [==============================] - 72s 383ms/step - loss: 0.0308 - accuracy: 0.9889 - val_loss: 1.6437 - val_accuracy: 0.7494\n",
      "Epoch 23/30\n",
      "188/188 [==============================] - 72s 384ms/step - loss: 0.0293 - accuracy: 0.9896 - val_loss: 1.7738 - val_accuracy: 0.7472\n",
      "Epoch 24/30\n",
      "188/188 [==============================] - 72s 385ms/step - loss: 0.0260 - accuracy: 0.9908 - val_loss: 1.8357 - val_accuracy: 0.7434\n",
      "Epoch 25/30\n",
      "188/188 [==============================] - 72s 385ms/step - loss: 0.0201 - accuracy: 0.9929 - val_loss: 1.9401 - val_accuracy: 0.7401\n",
      "Epoch 26/30\n",
      "188/188 [==============================] - 72s 385ms/step - loss: 0.0270 - accuracy: 0.9908 - val_loss: 1.8935 - val_accuracy: 0.7452\n",
      "Epoch 27/30\n",
      "188/188 [==============================] - 72s 385ms/step - loss: 0.0201 - accuracy: 0.9926 - val_loss: 1.9262 - val_accuracy: 0.7439\n",
      "Epoch 28/30\n",
      "188/188 [==============================] - 72s 384ms/step - loss: 0.0253 - accuracy: 0.9908 - val_loss: 1.9072 - val_accuracy: 0.7394\n",
      "Epoch 29/30\n",
      "188/188 [==============================] - 12539s 67s/step - loss: 0.0179 - accuracy: 0.9938 - val_loss: 2.0570 - val_accuracy: 0.7449\n",
      "Epoch 30/30\n",
      "154/188 [=======================>......] - ETA: 13s - loss: 0.0191 - accuracy: 0.9933"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(\n",
    "    loss=SparseCategoricalCrossentropy(),\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model2.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    validation_split=0.2,\n",
    "    #    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model2.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss}, Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5217c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
